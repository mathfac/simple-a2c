{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding another head to A2C: a next-state predictor. If we can train the model to accurately predict its own next state and reward, we could use it to generate additional training data. Inspired by how humans do \"mental practice\" by imagining scenarios in their head. Like that study with basketball players taking free throws: Those who practiced mentally performed better, even with same amount of \"live\" data. This sort of sample efficiency isn't really necessary when we have access to an env simulator, eg Gym, but could be very helpful for robotics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#N_STEPS = 5\n",
    "SEED = 1\n",
    "N_GAMES = 1000\n",
    "N_ACTIONS = 2\n",
    "N_INPUTS = 4\n",
    "\n",
    "states = []\n",
    "actions = []\n",
    "rewards = []\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.linear1 = nn.Linear(N_INPUTS, 64)\n",
    "        self.linear2 = nn.Linear(64, 128)\n",
    "        self.linear3 = nn.Linear(128, 64)\n",
    "        \n",
    "        self.actor = nn.Linear(64, N_ACTIONS)\n",
    "        self.critic = nn.Linear(64, 1)\n",
    "        self.predictor = nn.Linear(64, N_INPUTS)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.linear3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_action_probs(self, x):\n",
    "        x = self(x)\n",
    "        action_probs = F.softmax(self.actor(x))\n",
    "        return action_probs\n",
    "    \n",
    "    def evaluate_actions(self, x):\n",
    "        x = self(x)\n",
    "        action_probs = F.softmax(self.actor(x))\n",
    "        state_values = self.critic(x)\n",
    "        next_state = self.predictor(x)\n",
    "        \n",
    "        return action_probs, state_values, next_state\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    score = 0\n",
    "    done = False\n",
    "    env = gym.make('CartPole-v0')\n",
    "    state = env.reset()\n",
    "    global action_probs\n",
    "    while not done:\n",
    "        score += 1\n",
    "        s = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        \n",
    "        action_probs = model.get_action_probs(Variable(s))\n",
    "        \n",
    "        _, action_index = action_probs.max(1)\n",
    "        action = action_index.data[0] \n",
    "        next_state, reward, done, thing = env.step(action)\n",
    "        state = next_state\n",
    "        \n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActorCritic()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maksymets/Projects/fair/rl_workshop/rl/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "multinomial() missing 1 required positional arguments: \"num_samples\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-868fc40fed30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: multinomial() missing 1 required positional arguments: \"num_samples\""
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "num_games = []\n",
    "value_losses = []\n",
    "action_gains = []\n",
    "state_pred_losses = []\n",
    "\n",
    "for i in range(N_GAMES):\n",
    "    \n",
    "    del states[:]\n",
    "    del actions[:]\n",
    "    del rewards[:]\n",
    "    \n",
    "    state = env.reset() \n",
    "    done = False\n",
    "    \n",
    "    # act phase\n",
    "    while not done:\n",
    "        s = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        \n",
    "        action_probs = model.get_action_probs(Variable(s))\n",
    "        action = action_probs.multinomial().data[0][0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "    if True: #len(rewards) < 200: # only reflecting/training on episodes where a failure occured. No training\n",
    "        # signal in perfect games. \n",
    "        # Reflect phase\n",
    "        print(\"Training. Score was \", len(rewards))\n",
    "\n",
    "        R = []\n",
    "        rr = rewards\n",
    "        rr.reverse()\n",
    "\n",
    "        next_return = -30 #if len(rewards) < 200 else 1 # unnecessary now, should just be 0\n",
    "        # punish failure hard\n",
    "\n",
    "        for r in range(len(rr)):\n",
    "            this_return = rr[r] + next_return * .9\n",
    "            R.append(this_return)\n",
    "            next_return = this_return\n",
    "        R.reverse()\n",
    "\n",
    "        rewards = R\n",
    "        \n",
    "        \n",
    "        # taking only the last 20 states before failure. wow this really improves training\n",
    "        \"\"\"rewards = rewards[-20:]\n",
    "        states = states[-20:]\n",
    "        actions = actions[-20:]\"\"\"\n",
    "        \n",
    "        global ss\n",
    "        ss = Variable(torch.FloatTensor(states))\n",
    "        \n",
    "        global next_states\n",
    "        action_probs, state_values, next_states = model.evaluate_actions(ss)\n",
    "        \n",
    "        next_state_pred_loss = (ss[1:] - next_states[:-1]).pow(2).mean()\n",
    "\n",
    "        action_log_probs = action_probs.log() \n",
    "\n",
    "        advantages = Variable(torch.FloatTensor(rewards)).unsqueeze(1) - state_values\n",
    "\n",
    "        entropy = (action_probs * action_log_probs).sum(1).mean()\n",
    "\n",
    "        a = Variable(torch.LongTensor(actions).view(-1,1))\n",
    "\n",
    "        chosen_action_log_probs = action_log_probs.gather(1, a)\n",
    "\n",
    "        action_gain = (chosen_action_log_probs * advantages).mean()\n",
    "\n",
    "        value_loss = advantages.pow(2).mean()\n",
    "        \n",
    "        total_loss = value_loss/50.0 - action_gain - 0.0001*entropy + next_state_pred_loss\n",
    "        #total_loss = next_state_pred_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print(\"\\nRewards\", rewards, \"\\nState values\",  state_values, )\n",
    "        \n",
    "    else: print(\"Not training, score of \", len(rewards))\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        s = test_model(model)\n",
    "        scores.append(s)\n",
    "        num_games.append(i)\n",
    "\n",
    "        action_gains.append(action_gain.data.numpy()[0])\n",
    "        value_losses.append(value_loss.data.numpy()[0])\n",
    "        state_pred_losses.append(next_state_pred_loss.data.numpy()[0])\n",
    "\n",
    "        \n",
    "plt.plot(num_games, scores)\n",
    "plt.xlabel(\"N_GAMES\")\n",
    "plt.ylabel(\"Score\")\n",
    "#plt.title(EXP)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(num_games, value_losses)\n",
    "plt.xlabel(\"N_GAMES\")\n",
    "plt.ylabel(\"Value loss\")\n",
    "#plt.savefig(\"experiments/\"+EXP_NAME+'/'+EXP)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(num_games, action_gains)\n",
    "plt.xlabel(\"N_GAMES\")\n",
    "plt.ylabel(\"action gains\")\n",
    "#plt.savefig(\"experiments/\"+EXP_NAME+'/'+EXP)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(num_games, state_pred_losses)\n",
    "plt.xlabel(\"N_GAMES\")\n",
    "plt.ylabel(\"next state pred losses\")\n",
    "#plt.savefig(\"experiments/\"+EXP_NAME+'/'+EXP)\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ss[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_states[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6.18 \n",
    "\n",
    "Only taking last 20 frames really helps training. Otherwise spiky up and down, presumably bc interesting data is overpowered by no-signal data.\n",
    "\n",
    "Adding in next frame prediction.\n",
    "\n",
    "Simply adding next frame prediction loss to total loss doesn't seem to impede training (haven't tested scientifically)\n",
    "\n",
    "value loss is of much higher magnitude than other losses. Fiddling with the weights to see if it speeds training.\n",
    "\n",
    "Divide value loss by 50. Result: Doesn't seem to affect training? haven't tested scientifically. Investigate further.\n",
    "\n",
    "Predictor is training, though not perfectly. Actor Critic learns too fast to allow proper tuning (bc we're not training on batches w no failure). Let's include only prediction loss and see how good a predictor we can get.\n",
    "\n",
    "Only updating based on predictor loss. This seems to improve score? How would that be possible? Restart kernal and try again. OK, verified that it does NOT improve scores, whew.\n",
    "\n",
    "Training on only policy loss blows up gradients. Try again with lower LR.\n",
    "\n",
    "Training on only value loss score does NOT improve scores. Thought it did but restarted kernal and it didn't. wtf maybe it does... OK it does. Seems to even off at around 100, though extremely variable. First goes up to 100 consistently, then alternates btwn 100 and 10 ish. Investigate further\n",
    "\n",
    "Training on predictor only does NOT improve score. It does of course improve predictor substantially. To be truly valuable, though, it needs to predict more states than just those directly before failure. \n",
    "\n",
    "Predictor loss decreases to .01 after 2000 games. \n",
    "\n",
    "TODO: \n",
    "1) try training weights for total loss composition\n",
    "2) add head to model to also predict rewards\n",
    "3) Use predictor to generate imagined data. Train on this data and see what it does to scores.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
